{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f65b673d",
   "metadata": {},
   "source": [
    "# Kaggle Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f01a265",
   "metadata": {},
   "source": [
    "## Describe Your Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f2c779",
   "metadata": {},
   "source": [
    "**URL:** https://www.kaggle.com/datasets/lantian773030/pokemonclassification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee95bd2",
   "metadata": {},
   "source": [
    "**Task:**\n",
    "\n",
    "#í¬ì¼“ëª¬ 149ë§ˆë¦¬ì˜ ì´ë¯¸ì§€ ë°ì´í„° ì…‹ì„ í‘ë°± ì´ë¯¸ì§€ë¡œ ë³€í™˜í•˜ì—¬ ë¶„ë¥˜(ê¸°ì¡´ datasetì—ì„œ Alolan Sandslashì˜ í´ë”ëŠ” ì‚­ì œ)\n",
    "#datasetì˜ classê°€ ë§ê³  image dataê°¯ìˆ˜ê°€ ì ê¸° ë•Œë¬¸ì— íš¨ìœ¨ì ì¸ í•™ìŠµì„ ìœ„í•˜ì—¬ transformì„ í†µí•œ image dataë³€í˜• ë° gray scaleë¡œì˜ ë³€í˜•ì„ í†µí•´ í•™ìŠµì˜ íš¨ìœ¨ì„ ë†’ì„\n",
    "#classê°„ data ê°¯ìˆ˜ê°€ ë¶ˆê· í˜•í•˜ë¯€ë¡œ ê·¸ì— ë”°ë¥¸ weightë¥¼ Loss ê³„ì‚°ì— ë¶€ê³¼í•´ ì¤Œìœ¼ë¡œì¨ í•™ìŠµ íš¨ìœ¨ì„ ë†’ì„\n",
    "#ë‹¨ìˆœ MLP(Multi-Layer Perceptron) ë°©ì‹ê³¼ ì´ë¯¸ì§€ trainì— ì“°ì´ëŠ” CNN(Convolutional Neural Network) ë°©ì‹ ë‘ê°€ì§€ ë°©ì‹ìœ¼ë¡œ í•™ìŠµì„ ì§„í–‰"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925b65df",
   "metadata": {},
   "source": [
    "**Datasets**\n",
    "\n",
    "* Train dataset: ì „ì²´ Datasetì˜ 80%ë¥¼ random split\n",
    "\n",
    "* Validation dataset: ì „ì²´ Datasetì˜ 10%\n",
    "\n",
    "* Test dataset: ì „ì²´ Datasetì˜ 10%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd627a2",
   "metadata": {},
   "source": [
    "**Features(x):**\n",
    "\n",
    "64x64 Grayscale Image data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07cad80b",
   "metadata": {},
   "source": [
    "#### **Target(y):**\n",
    "\n",
    "149 class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6df73f8",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ccb4a7",
   "metadata": {},
   "source": [
    "## Build Your Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50952245-0dc5-42ee-8e38-dd3f33dd0be9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msungwoobaek\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from torch.utils.data import Dataset\n",
    "from collections import Counter\n",
    "import wandb\n",
    "import random\n",
    "import math\n",
    "from torchvision import datasets, models, transforms\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575c8d37",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "486febaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'Midtermdata'\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize(size=(64,64), antialias=True), # ì´ë¯¸ì§€ì˜ ì‚¬ì´ì¦ˆë¥¼ 64x64ì˜ ì‚¬ì´ì¦ˆë¡œ Resize\n",
    "    transforms.Grayscale(num_output_channels=1), # ì»¬ëŸ¬ì´ë¯¸ì§€ë¥¼ í‘ë°± ì´ë¯¸ì§€ë¡œ ë³€í™˜\n",
    "    transforms.RandomHorizontalFlip(), # ì¢Œìš° ë°˜ì „ default 50%í™•ë¥ ë¡œ ë°˜ì „\n",
    "    transforms.RandomVerticalFlip(), # ìƒí•˜ ë°˜ì „ default 50%í™•ë¥ ë¡œ ë°˜ì „\n",
    "    transforms.AutoAugment(policy=transforms.autoaugment.AutoAugmentPolicy.IMAGENET, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "    # pytorch ë‚´ë¶€ì ìœ¼ë¡œ ì •ì˜í•˜ê³  ìˆëŠ” policyë¥¼ ì‚¬ìš©í•˜ì—¬ ìë™ìœ¼ë¡œ augmentation\n",
    "    transforms.ToTensor(), # Tensor í˜•íƒœë¡œ ë³€í™˜\n",
    "    transforms.Normalize([0.5],[0.5]) # ì •ê·œí™” \n",
    "])\n",
    "image_datasets = datasets.ImageFolder(path,data_transforms) \n",
    "train_size = int(0.8 * len(image_datasets)) # train dataì˜ size(ì´ dataì˜ 80%)\n",
    "validation_size = int(0.1 * len(image_datasets)) # validation dataì˜ size(ì´ dataì˜ 10%)\n",
    "test_size = len(image_datasets) - train_size - validation_size # test dataì˜ size\n",
    "train_dataset, validation_dataset, test_dataset = torch.utils.data.random_split(image_datasets, [train_size, validation_size, test_size])\n",
    "# image data setì„ randomí•˜ê²Œ train, validation, test datasetìœ¼ë¡œ split\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=16,shuffle = True, drop_last=True)\n",
    "validation_loader = torch.utils.data.DataLoader(dataset=validation_dataset,batch_size=4,shuffle = True, drop_last=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=image_datasets, batch_size=4,shuffle = True, drop_last=True)\n",
    "class_counts = Counter(train_dataset.dataset.targets)\n",
    "train_weights = 1. / torch.tensor(list(class_counts.values()), dtype=torch.float).cuda() # classë³„ ë¶ˆê· í˜•ì„ ë§ì¶”ê¸° ìœ„í•œ train weight\n",
    "class_counts = Counter(validation_dataset.dataset.targets)\n",
    "validation_weights = 1. / torch.tensor(list(class_counts.values()), dtype=torch.float).cuda() # classë³„ ë¶ˆê· í˜•ì„ ë§ì¶”ê¸° ìœ„í•œ validation weight\n",
    "class_counts = Counter(test_dataset.dataset.targets)\n",
    "test_weights = 1. / torch.tensor(list(class_counts.values()), dtype=torch.float).cuda() # classë³„ ë¶ˆê· í˜•ì„ ë§ì¶”ê¸° ìœ„í•œ test weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a5ee14",
   "metadata": {},
   "source": [
    "### Model Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40c15d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc_layer = nn.Sequential(\n",
    "            nn.Linear(in_features=64*64,out_features=512,bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=512,out_features=256,bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=256,out_features=149,bias=True),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.fc_layer(x)\n",
    "        return x\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv_layer = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1,out_channels=16,kernel_size=3,stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.MaxPool2d(kernel_size=3),\n",
    "            nn.Conv2d(in_channels=16,out_channels=32,kernel_size=3,stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3),\n",
    "            nn.Conv2d(in_channels=32,out_channels=64,kernel_size=3,stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        self.fc_layer = nn.Sequential(\n",
    "            nn.Linear(in_features=256, out_features=128, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.Linear(in_features=128, out_features=149,bias=True),\n",
    "            #nn.Softmax(dim=1)   \n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layer(x)\n",
    "        x = x.view(x.shape[0],-1)\n",
    "        x = self.fc_layer(x)\n",
    "        return x\n",
    "\n",
    "mlp = MLP().cuda()\n",
    "cnn = CNN().cuda()\n",
    "train_criterion = torch.nn.CrossEntropyLoss(weight=train_weights)\n",
    "validation_criterion = torch.nn.CrossEntropyLoss(weight=validation_weights)\n",
    "test_criterion = torch.nn.CrossEntropyLoss(weight=test_weights)\n",
    "mlp_optimizer = optim.Adagrad(mlp.parameters(), lr=0.01)\n",
    "cnn_optimizer = optim.Adagrad(cnn.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad9bc902-5ad1-496e-a66a-9125f597bc13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\SungwooBaek\\Downloads\\midterm\\wandb\\run-20231105_201256-vk6a32ix</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/sungwoobaek/Midterm_project/runs/vk6a32ix' target=\"_blank\">bright-glade-1</a></strong> to <a href='https://wandb.ai/sungwoobaek/Midterm_project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/sungwoobaek/Midterm_project' target=\"_blank\">https://wandb.ai/sungwoobaek/Midterm_project</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/sungwoobaek/Midterm_project/runs/vk6a32ix' target=\"_blank\">https://wandb.ai/sungwoobaek/Midterm_project/runs/vk6a32ix</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.run.save without any arguments is deprecated.Changes to attributes are automatically persisted.\n"
     ]
    }
   ],
   "source": [
    "def log_image_table(images, predicted, labels, probs):\n",
    "    \"Log a wandb.Table with (img, pred, target, scores)\"\n",
    "    # ğŸ Create a wandb Table to log images, labels and predictions to\n",
    "    table = wandb.Table(columns=[\"image\", \"pred\", \"target\"]+[f\"score_{i}\" for i in range(10)])\n",
    "    for img, pred, targ, prob in zip(images.to(\"cpu\"), predicted.to(\"cpu\"), labels.to(\"cpu\"), probs.to(\"cpu\")):\n",
    "        table.add_data(wandb.Image(img[0].numpy()*255), pred, targ, *prob.numpy())\n",
    "    wandb.log({\"predictions_table\":table}, commit=False)\n",
    "wandb.init(\n",
    "    project=\"Midterm_project\",    \n",
    ")\n",
    "wandb.run.name = 'MLP'\n",
    "wandb.run.save()\n",
    "n_steps_per_epoch = math.ceil(len(train_loader.dataset) /16)\n",
    "example_ct = 0\n",
    "step_ct = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e814c018",
   "metadata": {},
   "source": [
    "### Train Model & Select Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6dcbadfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss of 80 epoch, 50 index : 3.2046921253204346\n",
      "loss of 80 epoch, 100 index : 4.012154579162598\n",
      "loss of 80 epoch, 150 index : 3.847484588623047\n",
      "loss of 80 epoch, 200 index : 3.754336357116699\n",
      "loss of 80 epoch, 250 index : 3.688281774520874\n",
      "loss of 80 epoch, 300 index : 3.582340955734253\n",
      "loss of 80 epoch, 350 index : 3.8492190837860107\n",
      "loss of 80 epoch, 400 index : 3.5572404861450195\n",
      "loss of 80 epoch, 450 index : 5.179178714752197\n",
      "loss of 80 epoch, 500 index : 4.005454063415527\n",
      "loss of 80 epoch, 550 index : 2.923060894012451\n",
      "loss of 80 epoch, 600 index : 3.322357177734375\n",
      "loss of 80 epoch, 650 index : 3.510493040084839\n",
      "epoch 80 :  4.831412953032545\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(80):\n",
    "    mlp.train()\n",
    "    for index, (data, target) in enumerate(train_loader):\n",
    "        X = data.view(-1,64*64).cuda()\n",
    "        Y = target.cuda()\n",
    "        output = mlp(X)\n",
    "        loss = train_criterion(output, Y)\n",
    "\n",
    "        mlp_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        mlp_optimizer.step()\n",
    "\n",
    "        example_ct += len(data)\n",
    "        metrics = {\"train/mlp_train_loss\": loss,\n",
    "                   \"train/mlp_iter\": (index + 1 + (n_steps_per_epoch * epoch)),\n",
    "                   \"train/mlp_example_ct\":example_ct}\n",
    "        if index + 1 < n_steps_per_epoch:\n",
    "            wandb.log(metrics)\n",
    "\n",
    "        step_ct +=1\n",
    "\n",
    "        if (index+1) % 50 == 0:\n",
    "            print(\"loss of {} epoch, {} index : {}\".format(epoch+1, index+1, loss.item()))\n",
    "        \n",
    "    mlp.eval()\n",
    "    with torch.no_grad():\n",
    "        valid_loss = sum(validation_criterion(mlp(X.view(-1,64*64).cuda()), Y.cuda()) for X, Y in validation_loader)\n",
    "        metrics = {\"valid/mlp_valid_loss\": valid_loss/len(validation_loader),\n",
    "                   \"valid/mlp_epoch\": epoch+1}\n",
    "        wandb.log(metrics)\n",
    "    print(\"epoch\", epoch+1,\": \", valid_loss.item() / len(validation_loader))\n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4fd2a299-3487-4735-a2bc-ee2f3530cde4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.run.name = 'CNN'\n",
    "wandb.run.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6db2fb9-5d1d-4499-bbca-58be7c16714c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss of 3000 epoch, 50 index : 3.0963430404663086\n",
      "loss of 3000 epoch, 100 index : 3.269336223602295\n",
      "loss of 3000 epoch, 150 index : 2.1023497581481934\n",
      "loss of 3000 epoch, 200 index : 3.4918558597564697\n",
      "loss of 3000 epoch, 250 index : 3.0338401794433594\n",
      "loss of 3000 epoch, 300 index : 3.351933717727661\n",
      "loss of 3000 epoch, 350 index : 3.8153700828552246\n",
      "loss of 3000 epoch, 400 index : 2.6145122051239014\n",
      "loss of 3000 epoch, 450 index : 3.7367429733276367\n",
      "loss of 3000 epoch, 500 index : 2.742234706878662\n",
      "loss of 3000 epoch, 550 index : 2.9828402996063232\n",
      "loss of 3000 epoch, 600 index : 2.8723769187927246\n",
      "loss of 3000 epoch, 650 index : 2.9956963062286377\n",
      "epoch 3000 :  3.685441056652182\n"
     ]
    }
   ],
   "source": [
    "n_steps_per_epoch = math.ceil(len(train_loader.dataset) / 16)\n",
    "example_ct = 0\n",
    "step_ct = 0\n",
    "for epoch in range(1000):\n",
    "    cnn.train()\n",
    "    for index, (data, target) in enumerate(train_loader):\n",
    "        X = data.cuda()\n",
    "        Y = target.cuda()\n",
    "        \n",
    "        output = cnn(X)\n",
    "\n",
    "        loss = train_criterion(output, Y)\n",
    "\n",
    "        cnn_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        cnn_optimizer.step()\n",
    "\n",
    "        example_ct += len(data)\n",
    "        metrics = {\"train/cnn_train_loss\": loss,\n",
    "                   \"train/cnn_iter\": (index + 1 + (n_steps_per_epoch * epoch)),\n",
    "                   \"train/cnn_example_ct\":example_ct}\n",
    "        if index + 1 < n_steps_per_epoch:\n",
    "            wandb.log(metrics)\n",
    "\n",
    "        step_ct +=1\n",
    "\n",
    "        if (index+1) % 50 == 0:\n",
    "            print(\"loss of {} epoch, {} index : {}\".format(epoch+1, index+1, loss.item()))\n",
    "        \n",
    "    cnn.eval()\n",
    "    with torch.no_grad():\n",
    "        valid_loss = sum(validation_criterion(cnn(X.cuda()), Y.cuda()) for X, Y in validation_loader)\n",
    "        metrics = {\"valid/cnn_valid_loss\": valid_loss.item() / len(validation_loader),\n",
    "                   \"valid/cnn_epoch\": epoch+1}\n",
    "        wandb.log(metrics)\n",
    "    print(\"epoch\", epoch+1,\": \", valid_loss.item() / len(validation_loader))\n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2dea6f0",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8779c8f2",
   "metadata": {},
   "source": [
    "## Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f49b4961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 13465.8672, Accuracy: 1208/13559 (9%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mlp.eval()  # test case í•™ìŠµ ë°©ì§€ë¥¼ ìœ„í•¨\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "with torch.no_grad():\n",
    "  for data, target in test_loader:\n",
    "    data = data.view(-1,64*64).cuda()\n",
    "    target = target.cuda()\n",
    "    output = mlp(data)\n",
    "    test_loss += test_criterion(output, target).item() # sum up batch loss\n",
    "    pred = output.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n",
    "    correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8dc9528f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 10756.1345, Accuracy: 2254/13559 (17%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cnn.eval()  # test case í•™ìŠµ ë°©ì§€ë¥¼ ìœ„í•¨\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "with torch.no_grad():\n",
    "  for data, target in test_loader:\n",
    "    data = data.cuda()\n",
    "    target = target.cuda()\n",
    "    output = cnn(data)\n",
    "    test_loss += test_criterion(output, target).item() # sum up batch loss\n",
    "    pred = output.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n",
    "    correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "284275b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(mlp.state_dict(), \"pth/mlp.pth\")\n",
    "torch.save(cnn.state_dict(), \"pth/cnn.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa2652e",
   "metadata": {},
   "source": [
    "The results explains\n",
    "\n",
    "...."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
